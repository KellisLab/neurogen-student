{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APNfq3knhfZg"
   },
   "source": [
    "# How to train your DragoNN\n",
    "## Exploring convolutional neural network (CNN) architectures for simulated genomic data. \n",
    "\n",
    "Make sure you are using the **Tutorial 2** kernel. This will (hopefully) ensure you have the necessary packages to run everything in the notebook. You can change the kernel by clicking on the button showing the active kernel at the top-right of the notebook or with the \"Kernel\" drop down menu on the menu bar.\n",
    "\n",
    "As you run this, Tensorflow will spit out warnings and errors all over the place, which you can safely ignore.\n",
    "\n",
    "## Outline<a name='outline'>\n",
    "<ol>\n",
    "    <li><a href=#2>Key properties of regulatory DNA sequences</a></li>\n",
    "    <li><a href=#3>Learning to localize homotypic motif density</a></li>\n",
    "    <li><a href=#4>Getting simulation data</a></li>  \n",
    "    <li><a href=#4.5>Running DragoNN on your own data: starting with FASTA files</a></li>\n",
    "    <li><a href=#5>Defining CNN architecture</a></li>\n",
    "    <li><a href=#6>Single layer, multiple filter model</a></li>\n",
    "    <li><a href=#7>Model Interpretation</a></li>    \n",
    "    <li><a href=#8>A multi-layer DragoNN model</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading DragoNN's tutorial utilities and reviewing properties of regulatory sequence that transcription factors bind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWLcVFDXzihK"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager().update('notebook', {'limit_output': 250})\n",
    "# Making sure our results are reproducible\n",
    "from numpy.random import seed\n",
    "seed(1234)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e3bYhMBhfZr"
   },
   "source": [
    "## Key properties of regulatory DNA sequences <a name='2'>\n",
    "\n",
    "![sequence properties 1](https://github.com/kundajelab/dragonn/blob/master/paper_supplement/primer_tutorial_images/sequence_properties_1.jpg?raw=1)\n",
    "![sequence properties 2](https://github.com/kundajelab/dragonn/blob/master/paper_supplement/primer_tutorial_images/sequence_properties_2.jpg?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ga1AXeKXhfZs"
   },
   "source": [
    "## Learning to localize homotypic motif density <a name='3'>\n",
    "\n",
    "In this tutorial we will learn how to localize a homotypic motif cluster. We will simulate a positive set of sequences with multiple instances of a motif in the center and a negative set of sequences with multiple motif instances positioned anywhere in the sequence:\n",
    "\n",
    "![homotypic motif density localization](https://github.com/kundajelab/dragonn/blob/master/tutorials/tutorial_images/homotypic_motif_density_localization.jpg?raw=1)\n",
    "\n",
    "We will then train a binary classification model to classify the simulated sequences. To solve this task, the model will need to learn the motif pattern and whether instances of that pattern are present in the central part of the sequence.\n",
    "\n",
    "![classification task](https://github.com/kundajelab/dragonn/blob/master/tutorials/tutorial_images/homotypic_motif_density_localization_task.jpg?raw=1)\n",
    "\n",
    "We start by getting the simulation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5-27zcHhfZt"
   },
   "source": [
    "## Getting simulation data <a name='4'>\n",
    "\n",
    "DragoNN provides a set of simulation functions. We will use the `simulate_motif_density_localization()` function to simulate homotypic motif density localization. First, we obtain documentation for the simulation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwUscf6NzihY"
   },
   "outputs": [],
   "source": [
    "from dragonn.simulations import *\n",
    "print_simulation_info(\"simulate_motif_density_localization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CzKV_rDhfZw"
   },
   "source": [
    "Next, we define parameters for a TAL1 motif density localization in 1500bp long sequence, with 0.4 GC fraction, and 2-4 instances of the motif in the central 150bp for the positive sequences. We simulate a total of 3000 positive and 3000 negative sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CgzxAhShfZw"
   },
   "outputs": [],
   "source": [
    "motif_density_localization_simulation_parameters = {\n",
    "    \"motif_name\": \"TAL1_known4\",\n",
    "    \"seq_length\": 1500,\n",
    "    \"center_size\": 150,\n",
    "    \"min_motif_counts\": 2,\n",
    "    \"max_motif_counts\": 4, \n",
    "    \"num_pos\": 3000,\n",
    "    \"num_neg\": 3000,\n",
    "    \"GC_fraction\": 0.4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HpAcK0mhfZz"
   },
   "source": [
    "We get the simulation data by calling the `get_simulation_data()` function with the simulation name and the simulation parameters as inputs. 1000 sequences are held out for a test set, 1000 sequences for a validation set, and the remaining 4000 sequences are in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y8cJCmy9hfZ0"
   },
   "outputs": [],
   "source": [
    "simulation_data = get_simulation_data(\"simulate_motif_density_localization\",\n",
    "                                      motif_density_localization_simulation_parameters,\n",
    "                                      validation_set_size=1000, test_set_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-e-3Sk0Azihm"
   },
   "source": [
    "simulation_data provides training, validation, and test sets of input sequences X and sequence labels y. The inputs X are matrices with a one-hot-encoding of the sequences:\n",
    "<img src=\"https://github.com/kundajelab/dragonn/blob/master/tutorials/tutorial_images/one_hot_encoding.png?raw=1\" width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8fJ0c1Mzihm"
   },
   "source": [
    "Simulation data is an object. It contains an attribute called X_train that is a numpy array of 4 dimensions. We can call the `shape` function on `X_train` to get it's dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EuEbg6Ftzihn",
    "outputId": "9b8e2d15-4b8e-4886-b6de-b85cec234a06"
   },
   "outputs": [],
   "source": [
    "simulation_data.X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McjA51Uk2ehG"
   },
   "source": [
    "Here are the first 10bp of a sequence in our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8uGOwNHTziht",
    "outputId": "07e0bb65-003b-4f9f-8f15-457f2acbcdea"
   },
   "outputs": [],
   "source": [
    "#The first dimension indicates the index of the training samples. \n",
    "# The second dimension is 1, and is only necessary because we are \n",
    "# performing 2D convolutions. We could omit this \"dummy\" dimension if\n",
    "# we used 1D convolutions. \n",
    "# The third dimension indicates the base index. \n",
    "# The fourth dimension indicates the base pair channels: A,C,G,T. \n",
    "\n",
    "simulation_data.X_train[0, :, :10, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLupnMwqhfZ-"
   },
   "source": [
    "We can convert this one-hot-encoded matrix back into a DNA string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "05S6ivnThfZ-",
    "outputId": "750f6cf5-90a2-4160-808c-c012d8b85c5c"
   },
   "outputs": [],
   "source": [
    "from dragonn.utils import *\n",
    "get_sequence_strings(simulation_data.X_train)[0][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrtwHPxthfaA"
   },
   "source": [
    "Let's examine the shape of training, validation, and test matrices: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zzNTlHQmhfaB",
    "outputId": "26d0cd58-2e07-44ce-8080-e2ff91a3ec9a"
   },
   "outputs": [],
   "source": [
    "print(simulation_data.X_train.shape)\n",
    "print(simulation_data.y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J89jB_BFhfaE",
    "outputId": "790931f0-ccb1-4304-9a5f-b1284ee19365"
   },
   "outputs": [],
   "source": [
    "print(simulation_data.X_valid.shape)\n",
    "print(simulation_data.y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A6tHqiBEhfaH",
    "outputId": "c5d07446-a6b1-48d4-82f9-66b0caaabf60"
   },
   "outputs": [],
   "source": [
    "print(simulation_data.X_test.shape)\n",
    "print(simulation_data.y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2G6qRmZziiH"
   },
   "source": [
    "## Running DragoNN on your own data: Starting with FASTA files <a name='4.5'>\n",
    "\n",
    "If you are running DragoNN on your own data, you can provide data in FASTA sequence format. We recommend generating 6 fasta files for model training: \n",
    "* Training positives \n",
    "* Training negatives \n",
    "* Validation positives \n",
    "* Validation negatives \n",
    "* Test positives \n",
    "* Test negatives \n",
    "\n",
    "To indicate how this could be done, we export the one-hot-encoded matrices from **simulation_data** to a FASTA file, and then show how this fasta file could be loaded back to a one-hot-encoded matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GsA3TIC0ziiI"
   },
   "outputs": [],
   "source": [
    "from dragonn.utils import fasta_from_onehot\n",
    "\n",
    "#get the indices of positive and negative sequences in the training, validation, and test sets \n",
    "train_pos=np.nonzero(simulation_data.y_train==True)\n",
    "train_neg=np.nonzero(simulation_data.y_train==False)\n",
    "valid_pos=np.nonzero(simulation_data.y_valid==True)\n",
    "valid_neg=np.nonzero(simulation_data.y_valid==False)\n",
    "test_pos=np.nonzero(simulation_data.y_test==True)\n",
    "test_neg=np.nonzero(simulation_data.y_test==False)\n",
    "\n",
    "#Generate gzipped  fasta files -- it is always a good idea to gzip your fasta files. This is less \n",
    "# important for our tiny example files, but becomes more relevant as the size of the files increases. \n",
    "# The fasta_from_onehot function gzips output fasta files. \n",
    "fasta_from_onehot(np.expand_dims(simulation_data.X_train[train_pos],axis=1),\"X.train.pos.fasta.gz\")\n",
    "fasta_from_onehot(np.expand_dims(simulation_data.X_valid[valid_pos],axis=1),\"X.valid.pos.fasta.gz\")\n",
    "fasta_from_onehot(np.expand_dims(simulation_data.X_test[test_pos],axis=1),\"X.test.pos.fasta.gz\")\n",
    "\n",
    "fasta_from_onehot(np.expand_dims(simulation_data.X_train[train_neg],axis=1),\"X.train.neg.fasta.gz\")\n",
    "fasta_from_onehot(np.expand_dims(simulation_data.X_valid[valid_neg],axis=1),\"X.valid.neg.fasta.gz\")\n",
    "fasta_from_onehot(np.expand_dims(simulation_data.X_test[test_neg],axis=1),\"X.test.neg.fasta.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tNuyVpaziiK"
   },
   "source": [
    "Let's examine \"X.train.pos.fasta.gz\" to verify that it's in the standard gzipped FASTA format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f16lnk8cziiK",
    "outputId": "916a1677-936d-4338-fb62-ab40519d1977"
   },
   "outputs": [],
   "source": [
    "! zcat X.train.pos.fasta.gz | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGFUpDVPziiO"
   },
   "source": [
    "We can then load fasta format data to generate training, validation, and test splits for our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2F1CWZ8iziiO"
   },
   "outputs": [],
   "source": [
    "from dragonn.utils import encode_fasta_sequences\n",
    "X_train_pos=encode_fasta_sequences(\"X.train.pos.fasta.gz\")\n",
    "X_train_neg=encode_fasta_sequences(\"X.train.neg.fasta.gz\")\n",
    "X_valid_pos=encode_fasta_sequences(\"X.valid.pos.fasta.gz\")\n",
    "X_valid_neg=encode_fasta_sequences(\"X.valid.neg.fasta.gz\")\n",
    "X_test_pos=encode_fasta_sequences(\"X.test.pos.fasta.gz\")\n",
    "X_test_neg=encode_fasta_sequences(\"X.test.neg.fasta.gz\")\n",
    "\n",
    "X_train=np.concatenate((X_train_pos,X_train_neg),axis=0)\n",
    "X_valid=np.concatenate((X_valid_pos,X_valid_neg),axis=0)\n",
    "X_test=np.concatenate((X_test_pos,X_test_neg),axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vcSZAyGdziiQ"
   },
   "outputs": [],
   "source": [
    "y_train=np.concatenate((np.ones(X_train_pos.shape[0]),\n",
    "                        np.zeros(X_train_neg.shape[0])))\n",
    "y_valid=np.concatenate((np.ones(X_valid_pos.shape[0]),\n",
    "                        np.zeros(X_valid_neg.shape[0])))\n",
    "y_test=np.concatenate((np.ones(X_test_pos.shape[0]),\n",
    "                        np.zeros(X_test_neg.shape[0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYOwbRPqziiS"
   },
   "source": [
    "Now, having read in the FASTA files, converted them to one-hot-encoded matrices, and defined label vectors, we are ready to train our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giRBRUbEhfaK"
   },
   "source": [
    "# Defining the convolutional neural network model architecture  <a name='5'>\n",
    "\n",
    "A locally connected linear unit in a CNN model can represent a [position-specific scoring matrix (PSSM)](https://en.wikipedia.org/wiki/Position_weight_matrix) (a). A sequence PSSM score is obtained by multiplying the PSSM across the sequence, thresholding the PSSM scores, and taking the max (b). A PSSM score can also be computed by a CNN model with tiled, locally connected linear units, amounting to a convolutional layer with a single convolutional filter representing the PSSM, followed by ReLU thresholding and maxpooling (c).\n",
    "![dragonn vs pssm](https://github.com/kundajelab/dragonn/blob/master/tutorials/tutorial_images/dragonn_and_pssm.jpg?raw=1)\\\n",
    "\n",
    "By utilizing multiple convolutional layers with multiple convolutional filters, CNN's can represent a wide range of sequence features in a compositional fashion:\n",
    "![dragonn model figure](https://github.com/kundajelab/dragonn/blob/master/tutorials/tutorial_images/dragonn_model_figure.jpg?raw=1)\n",
    "\n",
    "We will use the deep learning library [keras](http://keras.io/) with the [TensorFlow](https://github.com/tensorflow/tensorflow) backend to generate and train the CNN models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BMEgqp-bhfaL",
    "outputId": "d9e4c3f3-2c18-4eee-e257-3e0545afa680"
   },
   "outputs": [],
   "source": [
    "#To prepare for model training, we import the necessary functions and submodules from keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dropout, Reshape, Dense, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adadelta, SGD, RMSprop;\n",
    "import keras.losses;\n",
    "from keras.constraints import maxnorm;\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.callbacks import EarlyStopping, History\n",
    "from keras import backend as K \n",
    "K.set_image_data_format('channels_last')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfS72LrxhfaN"
   },
   "source": [
    "# Single layer, multi-filter model <a name='6'>\n",
    "\n",
    "We define a simple DragoNN model with one convolutional layer with 15 convolutional filters, followed by maxpooling of width 35. \n",
    "\n",
    "The model parameters are: \n",
    "\n",
    "* Input sequence length 1500 \n",
    "* 15 filter: there are neurons that act as  local pattern detectors on the input profile. \n",
    "* Convolutional filter width =  10: this metric defines the dimension of the filter weights; the model scans the entire input profile for a particular pattern encoded by the weights of the filter. \n",
    "* Max pool of width 35: computes the maximum value per-channel in sliding windows of size 35. We add the pooling layer becase DNA sequences are typically sparse in terms of the number of positions in the sequence that harbor TF motifs. The pooling layer allows us to reduce the size of the output profile of convolutional layers by employing summary statistics. \n",
    "\n",
    "![simArch1Layer](https://github.com/kundajelab/dragonn/blob/master/tutorials/tutorial_images/SimArch1Layer.png?raw=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ikMRDXRhfaO",
    "outputId": "7951caea-779e-4e79-f3ad-d3c962d86592"
   },
   "outputs": [],
   "source": [
    "#Define the model architecture in keras\n",
    "multi_filter_keras_model=Sequential() \n",
    "multi_filter_keras_model.add(Conv2D(filters=15,kernel_size=(1,10),input_shape=simulation_data.X_train.shape[1::]))\n",
    "multi_filter_keras_model.add(BatchNormalization(axis=-1))\n",
    "multi_filter_keras_model.add(Activation('relu'))\n",
    "multi_filter_keras_model.add(MaxPooling2D(pool_size=(1,35), strides=35))\n",
    "multi_filter_keras_model.add(Flatten())\n",
    "multi_filter_keras_model.add(Dense(1))\n",
    "multi_filter_keras_model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
    "multi_filter_keras_model.compile(optimizer='adam',\n",
    "                               loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_JhL10vzhfaQ",
    "outputId": "5b7b7a3b-284b-454d-b748-7593fb7e213d"
   },
   "outputs": [],
   "source": [
    "multi_filter_keras_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TY_guZFnziif"
   },
   "source": [
    "\"Non-trainable params\" refers to Batch Normalization parameter whose weights don't get updated during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8rzKZVoNhfaS"
   },
   "outputs": [],
   "source": [
    "##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
    "multi_filter_keras_model.compile(optimizer='adam',\n",
    "                               loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xwSZhvVhfaV"
   },
   "source": [
    "We train the model for 150 epochs, with an early stopping criterion -- if the loss on the validation set does not improve for 3 consecutive epochs, the training is halted. In each epoch, the model performs a complete pass over the training data, and updates its parameters to minimize the loss, which quantifies the error in the model predictions. After each epoch, the performance metrics for the model on the validation data were stored. \n",
    "\n",
    "The performance metrics include balanced accuracy, area under the receiver-operating curve ([auROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)), are under the precision-recall curve ([auPRC](https://en.wikipedia.org/wiki/Precision_and_recall)), and recall for multiple false discovery rates  (Recall at [FDR](https://en.wikipedia.org/wiki/False_discovery_rate))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GjRCE1JJhfaV"
   },
   "outputs": [],
   "source": [
    "from dragonn.callbacks import * \n",
    "#We define a custom callback to print training and validation metrics while training. \n",
    "metrics_callback=MetricsCallback(train_data=(simulation_data.X_train,simulation_data.y_train),\n",
    "                                 validation_data=(simulation_data.X_valid,simulation_data.y_valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u11Aj6ko2Gyv"
   },
   "source": [
    "We now proceed to train the model. We do this with the keras \"fit\" function. The \"fit\" function has a few key parameters: \n",
    "\n",
    "* **batch_size** -- the number of training and validation samples to be propagated through the network simultaneously. \n",
    "* **epochs** -- An epoch is a measure of the number of times all of the training vectors are used once to update the weights. For batch training all of the training samples pass through the learning algorithm simultaneously in one epoch before weights are updated.\n",
    "* **callbacks** -- Keras callbacks return information from a training algorithm while training is taking place. A callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training.\n",
    "* **EarlyStopping** -- a Keras callback that gets called at the end of each epoch. If the loss has not decreased for a consecutive n epochs, where n is referred to as the patience, the training is interrupted. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eJT2c7XFhfaX",
    "outputId": "8fc82857-57d1-4595-e9b5-be01fb6b6d33"
   },
   "outputs": [],
   "source": [
    "## use the keras fit function to train the model for 150 epochs with early stopping after 3 epochs \n",
    "history_multi_filter=multi_filter_keras_model.fit(x=simulation_data.X_train,\n",
    "                                  y=simulation_data.y_train,\n",
    "                                  batch_size=128,\n",
    "                                  epochs=150,\n",
    "                                  verbose=1,\n",
    "                                  callbacks=[EarlyStopping(patience=3,restore_best_weights=True),\n",
    "                                            metrics_callback],\n",
    "                                  validation_data=(simulation_data.X_valid,\n",
    "                                                   simulation_data.y_valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSmch4Hghfaa"
   },
   "source": [
    "### Evaluate the model on the held-out test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BCCsY5aohfab",
    "outputId": "0245c50e-7e1e-4b7d-bd1c-446a2eaaca35"
   },
   "outputs": [],
   "source": [
    "## Use the keras predict function to get model predictions on held-out test set. \n",
    "test_predictions=multi_filter_keras_model.predict(simulation_data.X_test)\n",
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "print(ClassificationResult(simulation_data.y_test,test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSYLNtjjhfaf"
   },
   "source": [
    "### Visualize the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSbvymo7ziiu"
   },
   "outputs": [],
   "source": [
    "#import functions foro visualization of data \n",
    "%matplotlib inline\n",
    "from dragonn.vis import *\n",
    "\n",
    "plot_learning_curve(history_multi_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0UFcjPMziiy"
   },
   "source": [
    "We can see that the training and validation loss decrease, but the validation loss is somewhat higher than the training loss. This is indicative of over-fitting to the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaBJrS2Qhfal"
   },
   "source": [
    "## Visualize the learned parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWMTRBqJhfal"
   },
   "source": [
    "Next, let's visualize the filter learned in this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u71ItwHBhfan"
   },
   "source": [
    "### Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "id": "Vwx8sbtuhfan",
    "outputId": "073dfdce-a39c-4e46-8763-e2d69f021efb"
   },
   "outputs": [],
   "source": [
    "plot_model_weights(multi_filter_keras_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9Ja44dfhfau"
   },
   "source": [
    "### Convolutional layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8v541M5mhfav"
   },
   "outputs": [],
   "source": [
    "W_conv, b_conv = multi_filter_keras_model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BuBSKMy-hfax",
    "outputId": "269e8ae7-c8e3-44b6-a089-41a271914179"
   },
   "outputs": [],
   "source": [
    "W_conv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fWVP0eEPhfay",
    "outputId": "657314ba-f866-4978-c75b-18b5d440082a"
   },
   "outputs": [],
   "source": [
    "b_conv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 800
    },
    "id": "kRjf1T--hfa0",
    "outputId": "d4efa15c-82f2-4d2a-a657-541464d92a59"
   },
   "outputs": [],
   "source": [
    "plot_filters(multi_filter_keras_model, simulation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8n_EMffXhfbC"
   },
   "source": [
    "# Model Interpretation <a name='7'>\n",
    "    \n",
    "As you can see, the filters/model parameters are difficult to be interepreted directly. However, there are alternative approaches of interepreting sequences.\n",
    "    \n",
    "Let's examine a positive and negative example from our simulation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Spz3zpGhfbE",
    "outputId": "dac6c2ed-86a6-4a2a-a052-09f9b28bbdfd"
   },
   "outputs": [],
   "source": [
    "#get the indices of the first positive and negative examples in the validation data split\n",
    "pos_indx=np.flatnonzero(simulation_data.y_valid==1)[0]\n",
    "print(pos_indx)\n",
    "pos_X=simulation_data.X_valid[pos_indx:pos_indx+1]\n",
    "\n",
    "neg_indx=np.flatnonzero(simulation_data.y_valid==0)[0]\n",
    "print(neg_indx)\n",
    "neg_X=simulation_data.X_valid[neg_indx:neg_indx+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnZ939dqhfbJ"
   },
   "source": [
    "### Motif Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ESoHCt9hfbJ"
   },
   "outputs": [],
   "source": [
    "from dragonn.utils import * \n",
    "pos_motif_scores=get_motif_scores(pos_X,simulation_data.motif_names,return_positions=True)\n",
    "neg_motif_scores=get_motif_scores(neg_X,simulation_data.motif_names,return_positions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "tAsR6VhphfbL",
    "outputId": "1aa6821e-4a94-4d19-e12c-b28672119bff"
   },
   "outputs": [],
   "source": [
    "from dragonn.vis import * \n",
    "plot_motif_scores(pos_motif_scores,title=\"Positive example\",ylim=(0,20))\n",
    "plot_motif_scores(neg_motif_scores,title=\"Negative example\",ylim=(0,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYLxiOEShfbP"
   },
   "source": [
    "The motif scan yields a group of three high-scoring motif alignment positions at a fixed distance near the center of the sequence in the positive example. The spacing of the high-scoring motif alignments is random in the negative sequence. \n",
    "\n",
    "Note: If you find that your negative example is too close to the positive examle (i.e. the randomly spaced motifs happen to have a spacing close to the positive example, feel free to provide another index value to select a different negative). \n",
    "\n",
    "For example, you can change the code to select a negative example to the below: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ILWmRtNJzijU",
    "outputId": "e2af40b1-a9a6-41ad-8698-02538a118169"
   },
   "outputs": [],
   "source": [
    "neg_indx=np.flatnonzero(simulation_data.y_valid==0)[10]\n",
    "print(neg_indx)\n",
    "neg_X=simulation_data.X_valid[neg_indx:neg_indx+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCfzueaWhfbQ"
   },
   "source": [
    "### *In silico* mutagenesis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAxE_amLhfbQ"
   },
   "source": [
    "To determine how much each position in the input sequence contrinbutes to the model's prediction, we can perform saturation mutagenesis on the sequence. For each position in the input sequence, we introduce each of the four possible bases A, C, G, T and quantify the effect on the model's predictions.\n",
    "\n",
    "*In silico* mutagenesis entails measuring the effect of a given base pair on the model's prediction of accessibility. The following algorithm is used: \n",
    "\n",
    "1. At each position in the input sequence, the reference allele is mutated to each of three possible alternate alleles, and the model predictions with the alternate alleles are obtained. \n",
    "\n",
    "2. The logit values for the reference allele are subtracted from the logit values for each of the 4 alleles. (This means that a difference of 0 will be obtained for the reference allele). We refer to these differences in logit at each position between the reference and alternate alleles as the ISM values. ISM values are computed in logit space to avoid any saturation effects from passing the logits through a sigmoid function. \n",
    "\n",
    "3. For each position, subtract the mean ISM value for that position from each of the 4 ISM values. \n",
    "\n",
    "4. Plot the 4xL heatmap of mean-normalized ISM values \n",
    "\n",
    "5. Plot the reference sequence bases weighted by the highest magnitude ISM score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ns_TksQkzijZ",
    "outputId": "80e4eb6a-81f7-43e5-a4ec-ed3a9f6ac001"
   },
   "outputs": [],
   "source": [
    "pos_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e-LN7IlUhfbQ",
    "outputId": "4b37d56b-27c7-4b42-8c30-4c71d5c4b969"
   },
   "outputs": [],
   "source": [
    "from dragonn.interpret.ism import *\n",
    "ism_pos=in_silico_mutagenesis(multi_filter_keras_model,pos_X,0)\n",
    "ism_neg=in_silico_mutagenesis(multi_filter_keras_model,neg_X,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 636
    },
    "id": "l6Vya1NBhfbT",
    "outputId": "81ae7a3e-a3e1-4d47-81b6-444ee50bc781",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from dragonn.vis import * \n",
    "\n",
    "# create discrete colormap of ISM scores \n",
    "#zoom into the central 150 bases of the sequence \n",
    "plot_ism(ism_pos,pos_X,title=\"Positive Example\",xlim=(675,825))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 636
    },
    "id": "GOh4lnLOzijg",
    "outputId": "303bdcb3-bebb-4550-c9f8-d9fe95573e18"
   },
   "outputs": [],
   "source": [
    "\n",
    "plot_ism(ism_neg,neg_X,title=\"Negative Example\",xlim=(675,825))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdX_rpL4zijh"
   },
   "source": [
    "We see clear TAL1 motif patterns emerging for the positive example in the central 150 bases of the input sequence; we do not see clear TAL1 patterns in the central 150 bases of the sequence for the negative example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sToN6KaqhfbX"
   },
   "source": [
    "### Gradient x Input \n",
    "\n",
    "Consider a neural net being a function: $f(x_1, ..., x_N; w) = y$\n",
    "\n",
    "One way to tell whether the input feature is important is to compute the gradient of the function with respect to (w.r.t.) model input: $\\frac{\\partial f}{\\partial x_i}$\n",
    "\n",
    "This approach is called saliency maps: \"Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\", by Karen Simonyan, Andrea Vedaldi and Andrew Zisserma https://arxiv.org/pdf/1312.6034.pdf\n",
    "\n",
    "In genomics, we typically visualize only gradients for bases observed in the sequence (called input masked gradients or input*grad)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Muro8H0Ahfba",
    "outputId": "5be00eca-0332-4e30-b24c-8a2d08fdec76"
   },
   "outputs": [],
   "source": [
    "from dragonn.interpret.input_grad import * \n",
    "gradinput_pos=input_grad(multi_filter_keras_model,pos_X)\n",
    "gradinput_neg=input_grad(multi_filter_keras_model,neg_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "0p64eg1Ghfbb",
    "outputId": "e587c62c-9717-4302-cdc2-dc6ca67e6226"
   },
   "outputs": [],
   "source": [
    "from dragonn.vis import plot_seq_importance\n",
    "plot_seq_importance(gradinput_pos,pos_X,title=\"Positive GradXInput\",xlim=(675,825))\n",
    "plot_seq_importance(gradinput_neg,neg_X,title=\"Negative GradXInput\",xlim=(675,825))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1_eQsEshfbg"
   },
   "source": [
    "This confirms what we observed with the ISM analysis -- the positive example contains TAL1 motifs in the central 150 base pairs; the negative example does not. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-V9EmMqhfbl"
   },
   "source": [
    "### DeepLIFT\n",
    "\n",
    "DeepLIFT is another approach to infer the contribution or importance of individual nucleotides in a specific input sequence to its predicted out. While gradients measure the sensitivity of the output to infinitesimal changes in the input, DeepLIFT scores quantify the sensitivity of the output to finite changes in the input. Specifically, the DeepLIFT algorithm backpropagates a score (analogous to gradients) which is based on comparing the activations of all the neurons in the network for the actual input sequence to those obtained when using neutral ‘reference’ sequences. We use dinucleotide-shuffled versions of any input sequence as reference sequences.\n",
    "\n",
    "[DeepLIFT](https://arxiv.org/pdf/1605.01713v2.pdf) allows us to obtain scores for specific sequence indicating the importance of each position in the sequence. DeepLIFT can accept a custom reference. For our purposes, we provide a dinucleotide-shuffled reference.\n",
    "\n",
    "We can now load the saved model for use in other applications or for further fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xoCK-8obhfbn",
    "outputId": "a5a134ec-dca1-47d4-e614-598f31a8baaf"
   },
   "outputs": [],
   "source": [
    "from dragonn.interpret.deeplift import * \n",
    "#note that the defaults for the deeplift function use 10 shuffled references per input sequence \n",
    "help(deeplift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_q16v7Qzijr"
   },
   "outputs": [],
   "source": [
    "### Saving a keras model \n",
    "\n",
    "We save the optimal regularized multi-layer keras model to an hdf5 file that contains both the model weights and architecture.\n",
    "\n",
    "multi_filter_keras_model.save(\"multi_filter_keras_model.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYUPpvChzijt"
   },
   "source": [
    "We can now load the saved model for use in other applications or for further fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6mO6_2Mzijt"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model=load_model(\"multi_filter_keras_model.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9EoqW-Zzijv"
   },
   "source": [
    "We first use the saved model to obtain the DeepLIFT scoring function. We use a shuffled reference, with 10 shuffled reference sequences for each example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6GNUM-Jzijv"
   },
   "outputs": [],
   "source": [
    "import dragonn\n",
    "from dragonn.interpret import * \n",
    "help(get_deeplift_scoring_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DgOfhw2Tzijz"
   },
   "outputs": [],
   "source": [
    "#target_layer_idx refers to the second-to-last model layer, which is the input to the sigmoid function\n",
    "#task_idx indicates that the first task in a multi-tasked model should be interpreted with DeepLIFT. Because \n",
    "# this is a single-tasked model, the task index will be 0 in all cases. \n",
    "dl_score_func=get_deeplift_scoring_function('multi_filter_keras_model.hdf5',\n",
    "                                           target_layer_idx=-2,\n",
    "                                           task_idx=0,\n",
    "                                           num_refs_per_seq=10,\n",
    "                                           reference='shuffled_ref',\n",
    "                                           one_hot_func=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHTWLXw2zij0"
   },
   "outputs": [],
   "source": [
    "#We use the scoring function to calculate deepLIFT scores for the positive and negative examples \n",
    "dl_pos=deeplift(dl_score_func,pos_X)\n",
    "dl_neg=deeplift(dl_score_func,neg_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBVn9Gdehfbo"
   },
   "outputs": [],
   "source": [
    "plot_seq_importance(dl_pos,pos_X,title=\"DeepLift positives\",xlim=(675,825))\n",
    "plot_seq_importance(dl_neg,neg_X,title=\"DeepLift negatives\",xlim=(675,825)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIR0L1BLhfbx"
   },
   "source": [
    "# A multi-layer DragoNN model <a name='8'>\n",
    "\n",
    "Next, we train a 3 layer model for this task. By adding additional layers, we allow the model to learn more complex features in the sequence data, such as the spatial constraint in the spacing of the TAL1 motifs. \n",
    "\n",
    "However, by adding additional layers to the model, we also make it more likely that the model will overfit to the training data -- we saw that the validation loss was higher than the training loss in the single-layer multi-filter model. We anticipate that this will only be more pronounced with a multi-filter model, so we add some regularization. We regularize the 3 layer using 0.2 dropout on every convolutional layer.\n",
    "![MultiLayerTraining](https://github.com/kundajelab/dragonn/blob/master/tutorials/tutorial_images/MultiLayerTraining.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aPFsBPq9hfby"
   },
   "outputs": [],
   "source": [
    "#Define the model architecture in keras\n",
    "\n",
    "regularized_keras_model=Sequential() \n",
    "regularized_keras_model.add(Conv2D(filters=15,kernel_size=(1,10),input_shape=simulation_data.X_train.shape[1::]))\n",
    "regularized_keras_model.add(Activation('relu'))\n",
    "regularized_keras_model.add(Dropout(0.2))\n",
    "\n",
    "regularized_keras_model.add(Conv2D(filters=15,kernel_size=(1,10),input_shape=simulation_data.X_train.shape[1::]))\n",
    "regularized_keras_model.add(Activation('relu'))\n",
    "regularized_keras_model.add(Dropout(0.2))\n",
    "\n",
    "regularized_keras_model.add(Conv2D(filters=15,kernel_size=(1,10),input_shape=simulation_data.X_train.shape[1::]))\n",
    "regularized_keras_model.add(Activation('relu'))\n",
    "regularized_keras_model.add(Dropout(0.2))\n",
    "regularized_keras_model.add(MaxPooling2D(pool_size=(1,35)))\n",
    "\n",
    "\n",
    "regularized_keras_model.add(Flatten())\n",
    "regularized_keras_model.add(Dense(1))\n",
    "regularized_keras_model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
    "regularized_keras_model.compile(optimizer='adam',\n",
    "                               loss='binary_crossentropy')\n",
    "\n",
    "regularized_keras_model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5m4Cdeu-zij8"
   },
   "outputs": [],
   "source": [
    "## use the keras fit function to train the model for 150 epochs with early stopping after 3 epochs \n",
    "history_regularized=regularized_keras_model.fit(x=simulation_data.X_train,\n",
    "                                  y=simulation_data.y_train,\n",
    "                                  batch_size=128,\n",
    "                                  epochs=150,\n",
    "                                  verbose=1,\n",
    "                                  callbacks=[EarlyStopping(patience=3,restore_best_weights=True),\n",
    "                                            History(),\n",
    "                                            metrics_callback],\n",
    "                                  validation_data=(simulation_data.X_valid,\n",
    "                                                   simulation_data.y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zGq5nOWCzij-"
   },
   "outputs": [],
   "source": [
    "## Use the keras predict function to get model predictions on held-out test set. \n",
    "test_predictions=regularized_keras_model.predict(simulation_data.X_test)\n",
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "print(ClassificationResult(simulation_data.y_test,test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zwPzaEfEzikA"
   },
   "outputs": [],
   "source": [
    "## Visualize the model's performance \n",
    "from dragonn.vis import * \n",
    "plot_learning_curve(history_regularized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBQBlhxyzikD"
   },
   "outputs": [],
   "source": [
    "regularized_keras_model.save(\"TAL1.Simulation.Regularized.3ConvLayers.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZohlnVOzikE"
   },
   "source": [
    "DragoNN provides a single utility function to interpret and plot model predictions through all the methodologies we have examined: \n",
    "    * Plots the motif scores (when available) to serve as a \"gold standard\" for interpretation\n",
    "    * Plots the ISM heatmap and sequence importance track \n",
    "    * Plots the gradient x input importance track\n",
    "    * Plots the DeepLIFT importance track \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HCQlVeONzikF"
   },
   "outputs": [],
   "source": [
    "from dragonn.interpret import *\n",
    "help(multi_method_interpret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTxsACJezikI"
   },
   "outputs": [],
   "source": [
    "#obtain the deepLIFT scoring function for interpretation \n",
    "dl_score_func_multi_layer_regularized=get_deeplift_scoring_function(\"TAL1.Simulation.Regularized.3ConvLayers.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nw2jPu_RzikJ"
   },
   "outputs": [],
   "source": [
    "from dragonn.vis import * \n",
    "pos_interpretations=multi_method_interpret(regularized_keras_model,\n",
    "                                           pos_X,\n",
    "                                           0,\n",
    "                                           dl_score_func_multi_layer_regularized,\n",
    "                                           motif_names=simulation_data.motif_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rrgPkENyzikL"
   },
   "outputs": [],
   "source": [
    "neg_interpretations=multi_method_interpret(regularized_keras_model,\n",
    "                                           neg_X,\n",
    "                                           0,\n",
    "                                           dl_score_func_multi_layer_regularized,\n",
    "                                           motif_names=simulation_data.motif_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e_CMcFwzikP"
   },
   "source": [
    "We now plot the interpretation scores for pos_X and neg_X along the full sequence as well as along the central 200 bp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1A_Zs-P8zikP"
   },
   "outputs": [],
   "source": [
    "plot_all_interpretations([pos_interpretations],pos_X,xlim=(675,825))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IkGRCvLPzikS"
   },
   "outputs": [],
   "source": [
    "plot_all_interpretations([neg_interpretations],neg_X,xlim=(675,825))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2vhSeq9hfcc"
   },
   "source": [
    "As expected, additional layers in combination with regularization via dropout lead to improved test set auPRC and decreased overfitting to the training set. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "PrimerTutorial 1 - Exploring model architectures for a homotypic motif density simulation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "dragonn",
   "language": "python",
   "name": "dragonn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
