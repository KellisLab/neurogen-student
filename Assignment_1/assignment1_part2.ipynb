{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54f3f94b",
   "metadata": {},
   "source": [
    "Problem Set 1.2: Introduction to Python with dimensionality reduction and classification.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe78e804",
   "metadata": {},
   "source": [
    "This is a Jupyter Notebook that executes commands in the Python programming language. Just like the R Markdown Notebook, when you execute the code in a cell the result will appear beneath the code. For a quick introduction to Python, see [here](https://cs231n.github.io/python-numpy-tutorial/).\n",
    "\n",
    "You can execute cells by placing your cursor inside a cell and clicking the *Run* button (play button) at the top of the notebook window, just beneath the tab bar or by pressing *Ctrl/Cmd++Enter*.\n",
    "\n",
    "You can add new chunk by clicking the *Insert Cell* (plus sign) button on the toolbar.\n",
    "\n",
    "Make sure you are using the **Assignment 1** kernel. This will (hopefully) ensure you have the necessary packages to run everything in the notebook. You can change the kernel by clicking on the button showing the active kernel at the top-right of the notebook or with the \"Kernel\" drop down menu on the menu bar.\n",
    "\n",
    "If the bash kernel does not appear as an option, open a new terminal (on the menu bar click *File* > *New* > *Terminal*) and type:\n",
    "\n",
    "`python -m bash_kernel.install`\n",
    "\n",
    "This should add the bash kernel as an option to Jupyter permanently.\n",
    "\n",
    "**When you are finished, save and download the notebook. You will need to submit it through Canvas once you have finished both sections.**\n",
    "\n",
    "Start by running this cell which will import the necessary packages and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a19ed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import mmread\n",
    "import sklearn.decomposition as decomp\n",
    "from sklearn.preprocessing import scale\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import os\n",
    "from pathlib import Path\n",
    "import urllib\n",
    "\n",
    "gcp_url = \"https://storage.googleapis.com/mit-cmn-neurogen-course-data/Assignment1\"\n",
    "write_path = \"data_assignment1\"\n",
    "Path(write_path).mkdir(parents=True, exist_ok=True)\n",
    "pset_dat = [\"p3_coldata_zQ_sc_single.tsv\", \"p3_counts_zQ_sc_single.mtx\", \"p3_rowdata_zQ_sc_single.tsv\",\n",
    "            \"p5_counts_zQ_sc_classification.mtx\", \"p5_labels_zQ_sc_classification.tsv\", \"p5_rowdata_zQ_sc_classification.tsv\"]\n",
    "for f in pset_dat:\n",
    "    urllib.request.urlretrieve(\"/\".join([gcp_url, f]), os.path.join(write_path, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0fc8e1",
   "metadata": {},
   "source": [
    "# Part 3: Dimensionality reduction with PCA\n",
    "\n",
    "Before we begin, you should familoarize yourself with the `numpy` package, which is the main python package for math and matrix manipulation. Refer to the tutorial, quickstart quide, and documentation here:\n",
    "\n",
    "\n",
    "1. [Numpy quickstart guide](https://numpy.org/doc/stable/user/quickstart.html)\n",
    "2. [Numpy documentation](https://numpy.org/doc/stable/)\n",
    "\n",
    "## Background\n",
    "The are ~6.2 billion bases in the human genome, any subset of which can be a feature of interest. Transcriptomics deals with studying the 50,000+ transcribed genes encoded by these bases, and epigenomics looks at the many more regulatory elements. A GWAS study may consider hundreds of thousands to millions of candidate single nucleotide polyphormisms for disease or trait associations. The common theme among genomic studies is that they are almost always high-dimensional, that is to say there are many features to account for, often far more than we are able or even care to. In the interest of time, resources, and interpretability we often look for ways to reduce our search space to focus on features of interest and relevance, but we don't ncessarily want to discard information. After all, when there are literally a million things to look at, how can we know in advance which ones matter or even how to begin proritizing them? \n",
    "\n",
    "Dimensionality reduction is a nethod for accomplishing exactly that. In essence, dimensionality reduction is the process by which we transform high-dimensional data into a much more palletable low-dimensional representation with minimal loss of information. The last part is critical. We don't want to lose anything of importance while shedding 95% of our data, but you might be surprised to know that it's actually easier done than said. Let's illustrate this with an example. A typical car has about 30,000 parts of which ~25,000 belong to the category of screws, grommets and plastic clips, several thousand of which are there just to hold things neatly in place while the car is assembled and could be stripped without affecting anything or without anyone noticing. Similarly, every mammalian cell expresses polymerases, histones, and ubiquitins, and while these may be more important than the zip ties in your car, more often than not they tell us equally as much about the cell's identity, functional specialization, or susceptibility in disease. Except under rare circumstances, these are examples of features that we can condense, if not outright discard. In doing so we not only have less features to worry about, but we know that whatever is left over is more important.\n",
    "\n",
    "There is a plethora of dimensionality reduction algorithms used in genomics, but we'll focus on one of most intuitive and practical ones called [*Principal Component Analysis (PCA)*](https://en.wikipedia.org/wiki/Principal_component_analysis). A thorough treatment of PCA is the subject of a linear algebra or machine learning course, but in brief, it is a method to transform a dataset of $n$ observations and $p>n$ features into a set of still $n$ observations but with at most $n-1$ new features called *components*. Every component vector is orthogonal to every other one and each explains some exclusive subset of the variance in your data. The components are sorted such that the first component explains the greatest amount of variance, the second component explains second most, and so on, such that all $n-1$ components explain 100% of the cross-sample variation present in your data. This reduced representation is not only more intuitive, it's more computationally tractable, especially when dealing with enormous datasets, and we can visualize the components to discover patterns that we would've otherwise missed.\n",
    "\n",
    "I didn't have time to put together some fancy figures illustrating the geometric intuition behind PCA, but when I Googled \"pca for dummies\" [I found someone who did](http://www.billconnelly.net/?p=697), although if you'd like a full blown tutorial there's [this](https://ourarchive.otago.ac.nz/bitstream/handle/10523/7534/OUCS-2002-12.pdf?sequence=1&isAllowed=y) as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec22c7e",
   "metadata": {},
   "source": [
    "## Computing the principal components\n",
    "\n",
    "In this section, we will apply PCA to a small, but respectably high-dimensionsal single-cell RNA-seq dataset. This data represents the transcriptional profiles of three cell types found in the striatum of a 6 month old mouse. We will use PCA to convert our minimally-processed gene count data into a much more compact form and use that reduced representation to identify the cell types present as well as which genes are responsible for driving their variation.\n",
    "\n",
    "Let's load the count matrix and cell metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c927cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cts = mmread(\"data_assignment1/p3_counts_zQ_sc_single.mtx\")\n",
    "cts = cts.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3a1322",
   "metadata": {},
   "source": [
    "Our count matrix is in the same format as in the previous section, where columns represent samples, and rows are genes. The only difference here is that the data was stored in sparse format. See example below for sparse vs dense data representations. \n",
    "\n",
    "This is because each sample corresponds to just one cell and these types of data ets usually range from thousands to millions of cells. However, since this particular dataset was sub-sampled to just a few hundred cells, we took the liberty of densifying it into a numpy array. We will deal extensively with sparse data structures in this course, but for now we will work exclusively with dense matrices since it will simplify things considerably."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce3dd3d",
   "metadata": {},
   "source": [
    "Let's practice doing simple mathematical operations on numpy arrays. We imported the `numpy` package as `np`, meaning that you can (and must) use this abbreviation to call functions belonging to the package (e.g. `np.mean()`). Column-normalize the counts just like you did before using the applicable numpy functions. For reference, here is the normalization formula again.\n",
    "$$ \\large\n",
    "\\mathbf{\\bar x_{j}} = M \\frac{\\mathbf{x_j}}{\\sum{\\mathbf{x_j}} }, \\quad j=1,...,n \n",
    "$$\n",
    "with\n",
    "$$ \\large\n",
    "M=\\text{median} \\bigg (\\sum{\\mathbf{x_1}}, ..., \\sum{\\mathbf{x_n}} \\bigg ) \\text.\n",
    "$$\n",
    "Assign the normalized array to `cts_norm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c11704",
   "metadata": {},
   "outputs": [],
   "source": [
    "cts_norm = ... # Fill this in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3193a5c4",
   "metadata": {},
   "source": [
    "Now something important to note is that normalization is not sufficient for the algorithm to work. For PCA to work properly, our features need to be centered and scaled to unit vectors. By convention, the matrix to be decomposed has observations as rows and features as columns, so we will first transpose our normalized counts and then center and scale the columns (now features). Th eresultant matrix to be passed to the algorithm is computed as\n",
    "\n",
    "$$ \\large\n",
    "    \\bar{X}^T = \\frac{X^T - \\mathbf{\\mu}_{\\text{feat}} }{\\mathbf{\\sigma_{\\text{feat}}}}\\text{.}\n",
    "$$\n",
    "\n",
    "This transforms our features to have a mean of 0 and standard deviation of 1. This type of matrix scaling (both of rows and columns) is common and a necessary preprocessing step in many contexts. You may not see it, but many tools you will use througout the course will perform this transformation under-the-hood before operating on the data. In the cell below implement this transformation and set the now transposed, centered, and scaled matrix to `Xs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5e2ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = ... # Fill this in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4196a02e",
   "metadata": {},
   "source": [
    "We could have also centered the matrix using the `scale` function from the `sklearn.preprocessing` module as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1f992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "Xt = cts_norm.transpose()\n",
    "Xs2 = scale(Xt, with_mean=True, with_std=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71d1df5",
   "metadata": {},
   "source": [
    "Let's verify whether the two transformations are equivalent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66d241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(Xs, Xs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea911570",
   "metadata": {},
   "source": [
    "The output of this will be almost (but not exactly) identical to `Xs`. You can consult the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html#sklearn.preprocessing.scale) for `scale` to learn why that is. However all down stream output will be, for the most part, indistinguishable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cbae03",
   "metadata": {},
   "source": [
    "We will use the sciki-learn package (`sklearn`). We imported the `sklearn.decomposition` module as `decomp`. This module contains the functions needed to perform PCA.  First initialize a `PCA` object from which we can call the fitting and transformation function to transform our preprocessed counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95587430",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomp.PCA()\n",
    "Xd = pca.fit_transform(Xs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918827cd",
   "metadata": {},
   "source": [
    "We now have our reduce representation `Xd`. We can check the size to see just how \"reduced\" our new representation is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa57edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Xt.shape)\n",
    "print(Xd.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba04029",
   "metadata": {},
   "source": [
    "We have consolidated our 3669 genes into 438 new features (components). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bb1058",
   "metadata": {},
   "source": [
    ">But wait, that's 1 component too many. Since we have only $n-1$ degrees of freedom, we should have $n-1=437$ components. So what exactly is the 438th component? Let's take a look. Recall that unlike R, Python is 0-indexed. So the 438th component is at index 437."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29a852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xd[:, 437][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89478267",
   "metadata": {},
   "source": [
    ">Our computer and numpy both use 64-bit percision, which allows us 16 digits of accuracy past the decimal point. Recall that we should have $n-1$ *non-zero* principal components, anything beyond would have to be a zero vector. As far as the computer is concerned, the 438th component is in fact all 0's."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6928a18",
   "metadata": {},
   "source": [
    "We also have the `pca` object. Let's examine it's attributes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e49e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.__dict__.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75862eec",
   "metadata": {},
   "source": [
    "Most interestingly the instance now contains:\n",
    "* `components_`: The variable loadings (sometimes called the *rotation matrix*)\n",
    "* `explained_variance_`:  The variance explined by each component\n",
    "* `explained_variance_ratio_`: The fraction of total variance explained by each component. \n",
    "\n",
    "Run the following cell, and you will see what percentage of the variance is explained by the first 2 components. Further, you can see the sum across all the components explains all of it. There is no loss of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e61137",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_[:2] * 100)\n",
    "print(np.sum(pca.explained_variance_ratio_) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41b567d",
   "metadata": {},
   "source": [
    "The first two components explain barely 9% of our variance. That doesn't seem like much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2d4c9e",
   "metadata": {},
   "source": [
    "## Finding patterns\n",
    "Let's load the metadata corresponding to the cells in count matrix that we started with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140975bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdata = pd.read_table(\"data_assignment1/p3_coldata_zQ_sc_single.tsv\", sep=\"\\t\", header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a836dfc",
   "metadata": {},
   "source": [
    "Now we'll plot the first two of the seemingly uninformative PC's we computed and label the points with the annotations in the meta data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d78f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = Xd[:, 0], y = Xd[:, 1], hue = mdata[\"SubType\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236f963a",
   "metadata": {},
   "source": [
    "So despite explaining only ~9% of the count data, the first two components are sufficient to partition the three cell populations present. This is actaully quite impressive. To appreciate why that is, let's provide some background on these cell types. This single-cell data consists of [oligodendrocytes](https://en.wikipedia.org/wiki/Oligodendrocyte) and [medium spiny neurons](https://en.wikipedia.org/wiki/Medium_spiny_neuron), also known as spiny projection neurons (SPN) as labeled here. Oligodendrocytes are a type of glial cell responsible for mylenating the axons of neurons enabling [action potentials](https://en.wikipedia.org/wiki/Action_potential) to travel much faster than they would otherwise. SPNs are a type of inhibitory neuron and the most abundant cell type in the striatum. There are two types of SPNs, distinguished by their expression of D1-type and D2-type dopamine receptors. The ones in this dataset are D1-expressing. Distinguishing between an inhibitory neuron and a glial cell should be fairly trivial and indeed it's where most of the variance lies. However, each type of SPN can be further subdivided in to striosomal SPNs and matrix SPNs. The striosomomal SPNs are localized into visible patches called [striosomes](https://en.wikipedia.org/wiki/Striosome) and are all believed to have a distinct functional role to those that reside in the \"matrix\" (the area outside the striosomes). Until very recently, it was nearly impossible to distinguish striosomal from matrix SPNs by anything other than their spatial localization, and there was even debate about whether they differed at all. Nevertheless, using this data we can clearly partition the two populations along an axis that supposedly only accounts for 0.7% of our variance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a17b1c",
   "metadata": {},
   "source": [
    "## Finding features\n",
    "Before saying anything about using dimensionality reduction to find patterns in data, we mentioned using it to narrow down features of interest. Indeed PCA is foremost a [*decomposition*](https://en.wikipedia.org/wiki/Matrix_decomposition) method, meaning that it decomposes a matrix into [two or more matrices](http://www.statistics4u.com/fundstat_eng/cc_pca_loadscore.html). What each of those matrices encodes depends on the method, and there are many. For PCA, the input matrix is decomposed into a matrix of principal components which encodes the observations into a reduced feature space, and a matrix of variable loadings, often called the rotation. The rotation matrix encodes the features into a new space as well, but not a reduced space necessarily. However, the magnitude of the values in this matrix tells us how much of an effect that feature has on each principal component and the sign tells us if the feature is positively or negatively correlated with the component. We can also do what we did previously and visually inspect out loadings to find patterns among the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58de0b21",
   "metadata": {},
   "source": [
    "Load the metadata corresponding to the features of the count matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb208d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdata = pd.read_table(\"data_assignment1/p3_rowdata_zQ_sc_single.tsv\", sep=\"\\t\", header=0)\n",
    "rdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef9ce89",
   "metadata": {},
   "source": [
    "Now let's visually inspect the first two loadings interactively using the `plotly` package. With this package you can generate a plot that allows you to zoom, pan, select, and see metadata about each individual point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c280bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "V = pca.components_.T # Extract and transpose the rotation matrix from the `pca` object. \n",
    "fig = px.scatter(rdata, x = V[:,0], y = V[:,1], color=rdata[\"marker\"],hover_name=rdata[\"Gene\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a00bec0",
   "metadata": {},
   "source": [
    "Using a different tool kit that we'll showcase later in the course, we identified the top 100 uniquely expressed marker genes for each of the cell types in this data and labeled them here accordingly. We can clearly see that by plotting the first two loadings, the genes in our data cluster by their selective expression in each of our cell types. The right corner of the cluster is heavily enriched for oligodendrocyte marker genes, many of which would be lowly expressed or entirely absent from neurons. As expected there is a much fuzzier boundary between the striosome and matrix SPN markers. The genes near the center line on the left which have no cell type association are either neuron-specific or SPN-specific genes that are not striosome or matrix enriched (e.g. *Nrg1*, *Nrxn3*, *Pde10a*, *Bcl11b*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b7828d",
   "metadata": {},
   "source": [
    "Last, part of decomposing a matrix implies that you can put it back together. Indeed we can multiply the PC's and the rotation matrix to recover the orignal (scaled) input matrix that we passed to the transformation function.\n",
    "\n",
    "$$ \\large\n",
    "\\bar{X} = X_{\\text{PC}}V\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071f2e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(Xs2, np.matmul(Xd, V.T))  # We need to un-invert V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d8e3bb",
   "metadata": {},
   "source": [
    "We can also use the conventient `inverse_transform()` method in the `PCA` class to do this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e4561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_org = ... # Fill this in\n",
    "np.allclose(Xs, X_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dff97b",
   "metadata": {},
   "source": [
    "We can even get our counts back if we were to revert the scaling and add back the means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d310420",
   "metadata": {},
   "source": [
    "## Discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b4f2fc",
   "metadata": {},
   "source": [
    "PCA is one of many dimensionality reduction methods used in genomics. Other commonly used methods include non-negative matrix factorization (NMF) and linear discriminant analysis (LDA) which , like PCA, are examples of linear methods, but there are also some that transform data into reduced non-linear representations such as autoencoders, and the popular UMAP and t-SNE visualization algorithms. They all have their particular use cases and data contexts in which they can be applied, but ultimately they all represent ways of condensing data in ways that make it easier to identify and extract interesting characteristics of our data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51447ac8",
   "metadata": {},
   "source": [
    "# Part 4: Clustering and classification\n",
    "We now move to the topics of deciding how to annotate, categorize, or otherwise desribe new data. When we conduct an experiment, especially one with a large number of samples or features, we are often given little information regarding where a data point came from, or what it represents. Even if we do, we are often left with the challenge of determining if and how the data points relate to one another. Dimensionality reduction and classification go hand in hand as one generally precedes the other. We may conduct an experiment with many variables, reduce and project our data to a low-dimensional space and behold... it's not random. There is structure, there are patterns, and we don't know what they are or why they're there. Since the second law of thermodynamics tells us that order does not generally occur in nature by chance (quite the opposite in fact) when we observe patterns in an experiment we are almost forced to assume that there is something interesting hiding in our data... or that we made a mistake. Either way, it's worth finding out what it is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894e4b53",
   "metadata": {},
   "source": [
    "Imagine you just finished an experiment and recieved your data. You run your favorite dimensionality reduction algorithm to get a 2D representation of your data (that we'll assume is correct), and when you plot it you see this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecad28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = data=np.random.rand(300,2)\n",
    "c = np.random.choice([0,1,2], 300)\n",
    "sns.scatterplot(x=R[:,0], y=R[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49b80ef",
   "metadata": {},
   "source": [
    "This looks like noise. You probably wouldn't be compelled to look much further.\n",
    "\n",
    "However, if you plotted your data and it looked like the one in the cell below, it might pique your interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424010d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "R, c = make_blobs(n_samples=500, centers=3, n_features=2, random_state=0,  cluster_std=0.25)\n",
    "sns.scatterplot(x=R[:,0], y=R[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aad4815",
   "metadata": {},
   "source": [
    "But this is too easy. Let's consider the case where the data isn't as neat, but it's clear that there is some structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c83ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "R_km, c_km = make_blobs(n_samples=500, centers=4, n_features=2, random_state=0, cluster_std=0.85)\n",
    "sns.scatterplot(x=R_km[:,0], y=R_km[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70583d7",
   "metadata": {},
   "source": [
    "We will attempt to partition and categorize the points in the unlabeled dataset using the [*k*-means](https://en.wikipedia.org/wiki/K-means_clustering) algorithm. \n",
    "The naive *k*-means algorithm works as follows:\n",
    "1. Randomly initalize *k* points the will serve as the centroids for classification.\n",
    "2. Assign each point to the cluster corresponding to the nearest centroid.\n",
    "3. Compute the new mean of the each cluster and make that the new centroid.\n",
    "4. Repeat 2 and 3 until the centroid positions no longer change. \n",
    "\n",
    "Since *k* is an input to the algorithm, it requires us to know the the numper of clusters *a priori* in order to give an adequate result. For this example, you can determine *k* by visual inspection.\n",
    "\n",
    "We're not going to manually implement *k*-means by hand. Although not at all difficult, it is time consuming and would not be of any educational value here. However, you will still be asked to do *k*-means clustering on this data. \n",
    "As part of the programming tutorial component of this assignemt, you will use the scikit-learn `KMeans()` implementation to classify the aforementioned dataset. `R_km` contains the *x* and *y* of the points you will be classifying. Refer to the [`KMeans()` documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) to implement the method. Assign the predicted labels to `c_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dded599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "c_pred = ... # Fill this in\n",
    "sns.scatterplot(x=R_km[:,0], y=R_km[:,1], hue=c_pred) # Plot your labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5f62ce",
   "metadata": {},
   "source": [
    "Now compare against the actual labels. They should look very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61172f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=R_km[:,0], y=R_km[:,1], hue=c_km)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933bd72f",
   "metadata": {},
   "source": [
    "*k*-means is an example of a *unsupervised* clustering algorithm. You may have heard the terms *supervised* and *unsupervised* thrown around in the context of machine learning. To put it in simple terms, an unsupervised algorithm operates on the input data exclusively and tries to learn the internal structure. Conversly, supervised algorithms learn by being *trained*, meaning they they are exposed to a labeled dataset that is used to \"learn the pattern\" so to speak and generate a model that can then be applied to make predictions about the labels of similar data.\n",
    "\n",
    "There exists a somewhat (distantly) related supervised counterpart to *k*-means called the *k*-nearest neighbor (*k*-NN) classifier. *k*-NN is a classification (not clustering) algorithm. There is a difference. A clustering algorithm places elements into categories, whereas a classification algorithm ask which category the element belongs to. There are instances in which either type can be applied to same task, but more often than not this is due to the user failing to understand the distinction. For our purposes, which will be discussed shortly, the classifier is better suited for the job.\n",
    "\n",
    "*k*-NN works by assigning each point in the test set the most common label among it's *k* neighbors in the training set. An interesting thing to note is that we don't even actually need a training set for *k*-NN if we have partially labeled data. In this case, the labeled points will effectively serve as the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb68533",
   "metadata": {},
   "source": [
    "Let's train *k*-NN on a similar toy dataset and test it on a noisier version of the data. Let's define the data generation parameters. This data set will have just 2 features and be grouped into 5 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7993a1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 500\n",
    "centers = 5\n",
    "seed = 0\n",
    "n_features = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5bb4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_train, c_train = make_blobs(n_samples=n_samples, centers=centers, n_features=n_features, random_state=seed,  cluster_std=0.7)\n",
    "sns.scatterplot(x=R_train[:,0], y=R_train[:,1], hue = c_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2735b98a",
   "metadata": {},
   "source": [
    "Now let's generate the test set. We will use the same data as above as  a base, but will increase the cluster variance and add randomly distributed noise to every point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a41ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_test, c_test = make_blobs(n_samples=n_samples, centers=centers, n_features=n_features, random_state=seed,  cluster_std=1.0)\n",
    "noise = np.random.normal(size=R_test.shape, scale = 0.1)\n",
    "R_test = R_test + noise\n",
    "sns.scatterplot(x=R_test[:,0], y=R_test[:,1], hue = c_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e036e5",
   "metadata": {},
   "source": [
    "Compare the training and test set. The latter is visibly noisier.\n",
    "\n",
    "Now lets load the class `KNeighborsClassifier`. The second and third lines will instantiate an object that will perforn *k*-NN by looking at the 5 nearest neighbors of every point and train a model on the test set `R_train` with ground truth labels `c_train`. The last line will generate the label predictions for the test data based on the model generated from the training set and assign the outpout to `c_pred_knn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e376eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh.fit(R_train, c_train)\n",
    "c_pred_knn = neigh.predict(R_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4166e336",
   "metadata": {},
   "source": [
    "Let's plot the classifier's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6111591",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=R_test[:,0], y=R_test[:,1], hue = c_pred_knn)\n",
    "print(np.sum(c_test == c_pred_knn)/len(c_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb2d197",
   "metadata": {},
   "source": [
    "The predictions were >90% correct and the the output looks quite similar to what we would expect from *k*-means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe92247",
   "metadata": {},
   "source": [
    "Let's do something more interesting. We will modify the data generation parameters to make our data more complex and high dimensional. This data will have 5 features and 7 clusters. We will continue plotting just the first 2 features for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c705c52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 500\n",
    "centers = 7\n",
    "seed = 2\n",
    "n_features = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57feee7",
   "metadata": {},
   "source": [
    "Now let's generate the new training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497a7af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_train, c_train = make_blobs(n_samples=n_samples, centers=centers, n_features=n_features, random_state=seed,  cluster_std=0.7)\n",
    "sns.scatterplot(x=R_train[:,0], y=R_train[:,1], hue = c_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543e5548",
   "metadata": {},
   "source": [
    "Now for the new test set. We're going to double the variance of both the clusters and the added noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b02181",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_test, c_test = make_blobs(n_samples=n_samples, centers=centers, n_features=n_features, random_state=seed,  cluster_std=2.0)\n",
    "noise = np.random.normal(size=R_test.shape, scale = 0.2)\n",
    "R_test = R_test + noise\n",
    "sns.scatterplot(x=R_test[:,0], y=R_test[:,1], hue = c_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b0b85a",
   "metadata": {},
   "source": [
    "This data is visibly noisier, and mind you we're only looking at two of the five features.\n",
    "\n",
    "Now let's train the *k*-NN classifier on this data exactly the same way as before and plot the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a13024",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh2 = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh2.fit(R_train, c_train)\n",
    "c_pred_knn = neigh2.predict(R_test)\n",
    "sns.scatterplot(x=R_test[:,0], y=R_test[:,1], hue = c_pred_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b646fc64",
   "metadata": {},
   "source": [
    "Not terrible.\n",
    "\n",
    "Let's evaluate the prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802ad46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'k-NN accuracy: {:0.1f}%'.format(np.sum(c_test==c_pred_knn)/len(c_test)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436df083",
   "metadata": {},
   "source": [
    "This is pretty good given how gross the data looks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288348ca",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Now that you have (or maybe already had) some degree of familiarity with the concepts of dimensionality reduction, clustering, classification, and supervised and unsuperivsed algorithms, you can hopefully make an informed decision about when it's appropriate to use each one.\n",
    "\n",
    "While these actual algorithmic examples showcased here might seem like trivial and pedagological use cases, these algorithms are actually used with little to no modification in genomic data analysis, unlike the ones in the R section of the problem set. For example, *k*-NN, both as is and in other flavors, is used for cell type identification in single-cell omic data and works superbly well. So much so that our lab and others have published variations of it tailored for this purpose. On the other hand, *k*-means works very poorly because cell type identification is a classification, not a clustering, problem. However this hasn't stopped people from using it. This is why it's important to understand the differences between different algorithmic paradigms and in what contexts to apply them.\n",
    "\n",
    "Now for the actual homework assignment..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce998c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 5: I promise it's the last one\n",
    "\n",
    "In this last problem, you will apply the concepts from these sections to classify cells as healthy or sick. Here we have provided a snRNA-seq dataset from a mouse model of Huntington's disease, the same one as from part 3. This dataset contains >1000 *Drd2*-expressing striosomal SPNs from four mice, roughly half of which are from 6 month old zQ175DN HD model mice and the other half from same-age C57BL/6 control litter mates. This is an interesting data set because despite this mouse model expressing mutant Huntingtin with 175 CAG repeats, it has a very mild motor phenotype. In contrast, the R6/2 model from part 2 had 160 repeats and at only 9 weeks of age exhibited a phenotype so severe, that they would not have survived more than a few more weeks. The biology underlying this difference is a very interesting topic of research and may be discussed in the neurodegeneration module of the course.  The transcriptional phenotype of the zQ175 is also fairly subtle, which adds to the  challange.\n",
    "\n",
    "We briefly considered providing human data for this problem, but that would have been too difficult. As you can see from [this publication](https://www.cell.com/neuron/pdf/S0896-6273(20)30475-X.pdf), even we had difficulty telling them apart.\n",
    "\n",
    "This problem will be graded primarily on how well you can justify your choice of classification approach moreso than the accuracy of the labels, provided that the labels are at least somehwat correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7d002e",
   "metadata": {},
   "source": [
    "We provided you with the following data:\n",
    "* `cts_class`: A sparse matrix containing the gene counts in feature-by-cell format. No preprocessing has been done other than reducing the number of genes to only those that are highly expressed.\n",
    "* `gene_data`: A data frame containing the ENSEMBL ID, gene name, chromosome, and biotype of each feature.\n",
    "* `df_label`: A data frame containing the labels, most of which are set to `NA`, sans 35 known labels which you can use as a starting point.\n",
    "\n",
    "As stated above, the `label` column containes the true labels of 35 cells selected at random. You may use these to determine the remainder of the missing labels.\n",
    "\n",
    "\n",
    "\n",
    "In the last cell at the bottom of this notebook, you are asked to explain your method and justify your approach.\n",
    "\n",
    "You can use any method you wish to determine the phenotypes, not just the classification and clustering methods discussed here. You may also use any dimensionality reduction and normalization scheme you wish, if you think you need them.\n",
    "\n",
    "Run this cell to load the problem data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2004bc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cts_class = mmread(\"data_assignment1/p5_counts_zQ_sc_classification.mtx\")\n",
    "gene_data = pd.read_table(\"data_assignment1/p5_rowdata_zQ_sc_classification.tsv\", sep=\"\\t\", header=0)\n",
    "df_label = pd.read_table(\"data_assignment1/p5_labels_zQ_sc_classification.tsv\", sep=\"\\t\", header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382a6553",
   "metadata": {},
   "source": [
    "Write your solution in the following cell. The output of your classifier should be a set of predictions that you will store in a new column named `label_pred` inside `df_label`. `df_label['idx']` contains the cell column indices, which should not be modified.\n",
    "Once you have your solution, make sure to run the subsequent cell which will write your solution to a TSV file and submit it with your notebook files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a297c263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_label['label_pred'] = ... # Fill this in\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0676564d-550b-47b6-bec7-0c32d8069648",
   "metadata": {},
   "source": [
    "Plot your result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbb8719-95f2-4672-ac1b-be47c1fdc4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill this in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f501fb",
   "metadata": {},
   "source": [
    "Save your result by running this cell. Feel free to change the file location, but keep the file name the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aa7d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label.to_csv('p5_predicted_labels.tsv', sep='\\t', na_rep='NA', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfd7ac5",
   "metadata": {},
   "source": [
    "Explain and justify your solution:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bebc4b80",
   "metadata": {},
   "source": [
    "Response: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49d30f7-df81-45f5-ac68-9c43f87825f3",
   "metadata": {},
   "source": [
    "**Remember to save and download the notebook, as well as your solution to part 5.**"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14915e85addbcad7d943d725d0ffddf08e085f67a7a11a8650759ba2c337b968"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
